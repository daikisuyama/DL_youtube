# ニューラルネットワークの学習の仕組み

## 時間

2/9 21時半ごろ

## ニューラルネットワークの学習とは？(復習)

重みは適当に乱数で初期化されており、出力が正解に近づくように結合の重みを更新していく。

## 学習データによる重みWの更新

乱数で初期化したパラメータWをミニバッチ"勾配降下法"で更新するのが一般的

1. 学習データの一部のデータをミニバッチとして取得
2. ミニバッチによりForward計算を行い、現在のパラメータWにより出力yと損失Lを求める
3. Backward計算(損失の逆伝播)を行い、パラメータWの勾配を求める
4. 求めた勾配を元に更新を行う($W^{'} \leftarrow - \eta \frac{dL}{dW}$)($\eta$は学習率)

これらを繰り返し行っていく

## 誤差逆伝播法

(ここ詳しく)(具体例しかなく弱い)

出力から逆順に遡って勾配を計算することで合成関数の微分を行うことができる

逆順に勾配を求めることでネットワーク全体で"パラメータに関する微分"を求めることができる(連鎖率)

