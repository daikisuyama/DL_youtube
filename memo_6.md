# ニューラルネットワークの多層化テクニック

## 時間

2/3 11時ごろ

## ニューラルネットワークの多層化について

ニューロン数と層を増やすことでネットワークの表現力は向上し、高い性能をはっきすることができる

- かつての問題：層を増やすほど学習が困難になる
    - 学習が収束しない
    - 層数を増やしても精度が伸びない
- 問題を解決するテクニック
    - 3層以上：ReLU,Dropout,より良い係数での初期化など
    - 10層以上：BatchNormalizationなどの正規化
    - 20層以上：ResNetのようなスキップコネクション,Auxiliary Loss など
    - (ここの用語を調べる)

## テクニックの紹介

(紹介していないものも調べる)

- ReLU
    - 基本的な活性化関数
    - 従来はTanhやSigmoidが用いられていたが、多層になると学習が困難になるため、ReLUが用いられるようになる
- Batch Normalization
    - ConvolutionやAffineの直後にBatchNormalizationを挿入する
    - BatchNormalizationにより、中間出力が平均0で分散1になるようにバッチ内での正規化を行う
    - これにより、安定した学習の収束と学習の加速が期待される
    - (要原著論文)
- ResNet
    - スキップコネクションを導入する
    - "誤差がネットワーク全体に効果的に伝播する"ことで深いネットワークの学習が可能になる
    - (要原著論文)